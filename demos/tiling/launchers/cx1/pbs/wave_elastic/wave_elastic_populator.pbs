#!/bin/bash

# Note: mpiexec.1pps runs hybrid mpi+openmp jobs
# Behind the scenes, it sets:
# I_MPI_PIN=yes
# I_MPI_PIN_MODE=lib
# I_MPI_PIN_DOMAIN=socket
# I_MPI_PIN_ORDER=compact
# KMP_AFFINITY=granularity=fine,compact,1,0

FIREDRAKE=$FIREDRAKE_DIR
TILING=$FIREDRAKE/demos/tiling
EXECUTABLE=$TILING/wave_elastic.py
MESHES=$WORK/meshes/wave_elastic

NODENAME=`cat $PBS_NODEFILE`
NODENAME="$( cut -d '.' -f 1 <<< "$NODENAME" )"

echo ------------------------------------------------------
echo -n 'Job is running on node '; echo $NODENAME
cat /proc/cpuinfo | grep "model name" | uniq
echo ------------------------------------------------------
echo PBS: qsub is running on $PBS_O_HOST
echo PBS: originating queue is $PBS_O_QUEUE
echo PBS: executing queue is $PBS_QUEUE
echo PBS: working directory is $PBS_O_WORKDIR
echo PBS: execution mode is $PBS_ENVIRONMENT
echo PBS: job identifier is $PBS_JOBID
echo PBS: job name is $PBS_JOBNAME
echo PBS: node file is $PBS_NODEFILE
echo PBS: current home directory is $PBS_O_HOME
echo PBS: PATH = $PBS_O_PATH
echo ------------------------------------------------------
echo PBS: PYTHONPATH = $PYTHONPATH
echo ------------------------------------------------------
echo PBS: SLOPE_BACKEND = $SLOPE_BACKEND
echo ------------------------------------------------------


export OMP_NUM_THREADS=1
export SLOPE_BACKEND=SEQUENTIAL

OPTS="--output 10000 --flatten True --nocache True --time_max 0.01 --tofile False"
TILE_OPTS="--fusion-mode only_tile --coloring default"

LOGGER=$WORK"/logger_"$PBS_JOBNAME"_multinode.txt"
rm -f $LOGGER
touch $LOGGER


declare -a opts_em1=("" "--glb-maps True")
declare -a opts_em2=("" "--glb-maps True")
declare -a opts_em3=("--glb-maps True")
declare -a opts_em4=("--glb-maps True" "--glb-maps True --extra-halo 1")
declare -a opts_em5=("--glb-maps True" "--glb-maps True --extra-halo 1")

declare -a part_all=("chunk")

declare -a mesh_p1=("--mesh-size (50.0,25.0,$mesh)")
declare -a mesh_p2=("--mesh-size (30.0,15.0,$mesh)")
declare -a mesh_p3=("--mesh-size (30.0,15.0,$mesh)")
declare -a mesh_p4=("--mesh-size (30.0,15.0,$mesh)")

declare -a em_all=(1 2 3 4 5)

export FIREDRAKE_FFC_KERNEL_CACHE_DIR=$TMPDIR/ffc-cache
export PYOP2_CACHE_DIR=$TMPDIR/pyop2-cache

MPICMD="mpiexec -env FIREDRAKE_FFC_KERNEL_CACHE_DIR $FIREDRAKE_FFC_KERNEL_CACHE_DIR -env PYOP2_CACHE_DIR $PYOP2_CACHE_DIR"

# Populate the local cache
for poly in ${polys[@]}
do
    if [ -n "$onlylog" ]; then
        break
    fi
    echo "Populate polynomial order "$poly >> $LOGGER
    mesh_p="mesh_p$poly[@]"
    meshes=( "${!mesh_p}" )
    for mesh in "${meshes[@]}"
    do
        echo "    Populate "$mesh >> $LOGGER
        echo "        Populate Untiled ..." >> $LOGGER
        $MPICMD python $EXECUTABLE --poly-order $poly $mesh $OPTS --num-unroll 0
        for p in ${part_all[@]}
        do
            for em in ${em_all[@]}
            do
                opts="opts_em$em[@]"
                opts_em=( "${!opts}" )
                for opt in "${opts_em[@]}"
                do
                    ts=100000
                    echo "        Populate Tiled (pm="$p", ts="$ts", em="$em") ..." >> $LOGGER
                    $MPICMD python $EXECUTABLE --poly-order $poly $mesh $OPTS --num-unroll 1 --tile-size $ts --part-mode $p --explicit-mode $em $TILE_OPTS $opt
                done
            done
        done
    done
done

rm $LOGGER

# Copy the local cache to the shared file system
mkdir -p $HOME/cache/pyop2-cache
mkdir -p $HOME/cache/ffc-cache
cp $PYOP2_CACHE_DIR/* $HOME/cache/pyop2-cache
cp $FIREDRAKE_FFC_KERNEL_CACHE_DIR/* $HOME/cache/ffc-cache
